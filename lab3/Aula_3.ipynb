{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (1.26.1)\n",
            "Requirement already satisfied: pandas in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (2.1.2)\n",
            "Requirement already satisfied: sklearn in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (0.0.post11)\n",
            "Requirement already satisfied: matplotlib in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from matplotlib) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from matplotlib) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/harmonia/codes/machine-learning-laboratory-2023-2/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# install libraries\n",
        "!pip3 install numpy pandas sklearn matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFBi0JnRjUZv"
      },
      "source": [
        "#**Aula 3 de Labortório de Aprendizado de Máquina**\n",
        "#Naive bayes, árvores e k-means\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYYpFMFQlFsy"
      },
      "source": [
        "##Imports das bibliotecas principais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7eWDveiTi0_M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import naive_bayes\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "from sklearn import feature_selection\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import ensemble\n",
        "from sklearn import linear_model\n",
        "from sklearn import cluster\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg1p94wji3S-"
      },
      "source": [
        "##Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWLUa9QDjFL-"
      },
      "source": [
        "#**1) Para a base de dados Haberman (disponibilizada em anexo), faça:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsInEjqXqmfg"
      },
      "source": [
        "**a) Divida aleatoriamente a base de dados em duas partes: treino, com 80% das amostras, e teste com as restantes, sendo o atributo alvo a última coluna. Use a parte de treino para estimar um modelo de Naive Bayes e calcule a acurácia\n",
        "sobre o conjunto de teste. Considere o modelo que usa histograma e distribuição Gaussiana.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vTZHkyQKqlor"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>year of operation</th>\n",
              "      <th>number of nodes detected</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>306.000000</td>\n",
              "      <td>306.000000</td>\n",
              "      <td>306.000000</td>\n",
              "      <td>306.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>52.457516</td>\n",
              "      <td>62.852941</td>\n",
              "      <td>4.026144</td>\n",
              "      <td>1.264706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>10.803452</td>\n",
              "      <td>3.249405</td>\n",
              "      <td>7.189654</td>\n",
              "      <td>0.441899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>44.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>52.000000</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>60.750000</td>\n",
              "      <td>65.750000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>83.000000</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              age  year of operation  number of nodes detected       class\n",
              "count  306.000000         306.000000                306.000000  306.000000\n",
              "mean    52.457516          62.852941                  4.026144    1.264706\n",
              "std     10.803452           3.249405                  7.189654    0.441899\n",
              "min     30.000000          58.000000                  0.000000    1.000000\n",
              "25%     44.000000          60.000000                  0.000000    1.000000\n",
              "50%     52.000000          63.000000                  1.000000    1.000000\n",
              "75%     60.750000          65.750000                  4.000000    2.000000\n",
              "max     83.000000          69.000000                 52.000000    2.000000"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('haberman.data', header = None, names = ['age', 'year of operation', 'number of nodes detected', 'class'])\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MWUEZ9JXvlTi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia de 62 amostras usando categórico: 75%\n",
            "Acurácia de 62 amostras usando Gaussiana: 75%\n"
          ]
        }
      ],
      "source": [
        "#divide os dados\n",
        "target = data['class']\n",
        "X = data.copy()\n",
        "X = X.drop('class', axis=1)\n",
        "X_train,X_test = train_test_split(X,test_size=0.2, random_state=10)\n",
        "target_train,target_test = train_test_split(target,test_size=0.2, random_state=10) # o que é o random state?\n",
        "\n",
        "# dúvidas\n",
        "  # o que eh o target? É o atributo alvo para ser previsto\n",
        "  # \n",
        "\n",
        "#alpha = 1 --> modelo categórico(considera os valores dos atributos como bins em\n",
        "#um histograma) com suavização de Laplace\n",
        "model = naive_bayes.CategoricalNB(alpha=1)\n",
        "model.fit(X_train,target_train)\n",
        "target_pred = model.predict(X_test)\n",
        "print(\"Acurácia de %d amostras usando categórico: %d%%\"\n",
        "  % (X_test.shape[0], 100*(target_test == target_pred).sum()/X_test.shape[0]))\n",
        "\n",
        "#considerando atributos com distribuição Gaussiana\n",
        "model = naive_bayes.GaussianNB()\n",
        "model.fit(X_train,target_train)\n",
        "target_pred = model.predict(X_test)\n",
        "print(\"Acurácia de %d amostras usando Gaussiana: %d%%\"\n",
        "  % (X_test.shape[0], 100*(target_test == target_pred).sum()/X_test.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WLPbPo-yLf3"
      },
      "source": [
        "**b) Padronize os valores das variáveis por zscore, ou seja, subtraia a média e divida pelo desvio padrão. Compare\n",
        "o resultado com o da letra a) para distribuição Gaussiana. O que você observou?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o6iqZeewzagG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia de 62 amostras usando Gaussiana: 75%\n",
            "Acurácia de 62 amostras usando Gaussiana: 75%\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('haberman.data', header = None, names = ['age', 'year of operation', 'number of nodes detected', 'class'])\n",
        "\n",
        "target = data['class']\n",
        "X = data.copy()\n",
        "X = X.drop('class', axis=1)\n",
        "\n",
        "X_train,X_test = train_test_split(X,test_size=0.2, random_state=10)\n",
        "target_train,target_test = train_test_split(target,test_size=0.2, random_state=10)\n",
        "\n",
        "#Z-score\n",
        "mean = X_train.mean()\n",
        "std = X_train.std()\n",
        "\n",
        "X_train_Z = (X_train - mean)/std\n",
        "X_test_Z = (X_test - mean)/std\n",
        "\n",
        "#considerando atributos com distribuição Gaussiana\n",
        "model = naive_bayes.GaussianNB()\n",
        "model.fit(X_train_Z,target_train)\n",
        "target_pred = model.predict(X_test_Z)\n",
        "print(\"Acurácia de %d amostras usando Gaussiana: %d%%\"\n",
        "  % (X_test.shape[0], 100*(target_test == target_pred).sum()/X_test.shape[0]))\n",
        "\n",
        "# #Mudança de escala dos atributos 0 e 1\n",
        "# X.iloc[:,0] = 10*X.iloc[:,0] + 10\n",
        "# X.iloc[:,1] = 500*X.iloc[:,1] + 500\n",
        "# X_train,X_test = train_test_split(X,test_size=0.2, random_state=10)\n",
        "# target_train,target_test = train_test_split(target,test_size=0.2, random_state=10)\n",
        "\n",
        "# model = naive_bayes.GaussianNB()\n",
        "# model.fit(X_train,target_train)\n",
        "# target_pred = model.predict(X_test)\n",
        "# print(\"Acurácia de %d amostras usando Gaussiana: %d%%\"\n",
        "#   % (X_test.shape[0], 100*(target_test == target_pred).sum()/X_test.shape[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtPg7Ql-1g1u"
      },
      "source": [
        "**c) Adicione duas variáveis aleatórias às variáveis de entrada do modelo, tal que elas tenham distribuição Gaussiana, com\n",
        "médias e desvios padrões diferentes entre elas. Compare o resultado com o da letra a) para distribuição Gaussiana. O que você\n",
        "observou?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj-ZxY1l1ot4"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('haberman.data', header = None, names = ['age', 'year of operation', 'number of nodes detected', 'class'])\n",
        "\n",
        "target = data['class']\n",
        "X = data.copy()\n",
        "X = X.drop('class', axis=1)\n",
        "X['aleatorio_1'] = np.random.normal(size=X.shape[0])\n",
        "X['aleatorio_2'] = 10*np.random.normal(size=X.shape[0]) + 100\n",
        "\n",
        "X_train,X_test = train_test_split(X,test_size=0.2, random_state=10)\n",
        "target_train,target_test = train_test_split(target,test_size=0.2, random_state=10)\n",
        "\n",
        "#considerando atributos com distribuição Gaussiana\n",
        "model = naive_bayes.GaussianNB()\n",
        "model.fit(X_train,target_train)\n",
        "target_pred = model.predict(X_test)\n",
        "print(\"Acurácia de %d amostras usando Gaussiana: %d%%\"\n",
        "  % (X_test.shape[0], 100*(target_test == target_pred).sum()/X_test.shape[0]))\n",
        "\n",
        "X.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BHVXVxd65Of"
      },
      "source": [
        "**d) Verifique o efeito de alterar as probabilidades a priori de cada classe.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA0L66i064ql"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('haberman.data', header = None, names = ['age', 'year of operation', 'number of nodes detected', 'class'])\n",
        "\n",
        "target = data['class']\n",
        "X = data.copy()\n",
        "X = X.drop('class', axis=1)\n",
        "\n",
        "X_train,X_test = train_test_split(X,test_size=0.2, random_state=10)\n",
        "target_train,target_test = train_test_split(target,test_size=0.2, random_state=10)\n",
        "\n",
        "#considerando atributos com distribuição Gaussiana\n",
        "model = naive_bayes.GaussianNB(priors=[1/3,2/3])\n",
        "model.fit(X_train,target_train)\n",
        "target_pred = model.predict(X_test)\n",
        "print(\"Acurácia de %d amostras usando Gaussiana: %d%%\"\n",
        "  % (X_test.shape[0], 100*(target_test == target_pred).sum()/X_test.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZvoBrPM8eU1"
      },
      "source": [
        "##Árvores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27sBjaEh-KOW"
      },
      "source": [
        "#**2) Para a base de dados Balance (disponibilizada em anexo), faça:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFmPhQgF-gml"
      },
      "source": [
        "**a) Divida aleatoriamente a base de dados em duas partes: treino, com 80% das amostras, e teste com as restantes, sendo o atributo alvo a primeira coluna. Use a parte de treino para estimar um modelo de árvore de decisão e calcule a acurácia\n",
        "sobre o conjunto de teste. Visualize a árvore.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dngX6Bmo-PEk"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('balance-scale.data', header = None, names = ['class','Left-Weight', ' Left-Distance', 'Right-Weight', 'Right-Distance'])\n",
        "\n",
        "#divide os dados\n",
        "target = data['class']\n",
        "X = data.copy()\n",
        "X = X.drop('class', axis=1)\n",
        "X_train,X_test = train_test_split(X,test_size=0.2, random_state=0)\n",
        "target_train,target_test = train_test_split(target,test_size=0.2, random_state=0)\n",
        "\n",
        "model = tree.DecisionTreeClassifier(max_depth = 3)\n",
        "model.fit(X_train,target_train)\n",
        "target_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Acurácia de %d amostras usando árvore: %d%%\"\n",
        "  % (X_test.shape[0], 100*(target_test == target_pred).sum()/X_test.shape[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvLqaW7gAsAT"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
        "\n",
        "tree.plot_tree(model,\n",
        "               feature_names = ['Left-Weight', ' Left-Distance', 'Right-Weight', 'Right-Distance'],\n",
        "               class_names = np.unique(target_train),\n",
        "               rounded=True,\n",
        "               precision = 3,\n",
        "               filled = True);\n",
        "fig.savefig('tree.png')\n",
        "\n",
        "plt.plot('tree.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iCca0CQDvYL"
      },
      "source": [
        "**b) Utilize Random Forest para calcular a acurácia sobre os mesmos dados e compare com o resultado anterior.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU6kuPgzEubj"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('balance-scale.data', header = None, names = ['class','Left-Weight', ' Left-Distance', 'Right-Weight', 'Right-Distance'])\n",
        "\n",
        "#divide os dados\n",
        "target = data['class']\n",
        "X = data.copy()\n",
        "X = X.drop('class', axis=1)\n",
        "X_train,X_test = train_test_split(X,test_size=0.2, random_state=0)\n",
        "target_train,target_test = train_test_split(target,test_size=0.2, random_state=0)\n",
        "\n",
        "model = ensemble.RandomForestClassifier(max_depth=3, random_state=0, n_estimators=20)\n",
        "model.fit(X_train,target_train)\n",
        "target_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Acurácia de %d amostras usando Random Forest: %d%%\"\n",
        "  % (X_test.shape[0], 100*(target_test == target_pred).sum()/X_test.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44QjYlR6Fc3G"
      },
      "source": [
        "#**3 Avalie o SHAP values dos modelos para a base de dados California Housing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7z3DpXhF46R"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3jN83SuF7vg"
      },
      "source": [
        "**a) Divida os dados em 80% para treinar e 20% para testar um modelo de regressão linear**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6_2fpSpGTr6"
      },
      "outputs": [],
      "source": [
        "X,y = shap.datasets.california(display=True)\n",
        "X_train,X_test = train_test_split(X,test_size=0.2, random_state=0)\n",
        "y_train,y_test = train_test_split(y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X_train,y_train)\n",
        "target_pred = model.predict(X_test)\n",
        "print(f\"RMSE do modelo de regressão linear: {metrics.mean_squared_error(y_test,target_pred)}\")\n",
        "#model_param = list(model.coef_)\n",
        "#model_param.append(model.intercept_)\n",
        "#print(f'Parametros do modelo {model_param}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsouKSz8IYIS"
      },
      "outputs": [],
      "source": [
        "X_shap = shap.utils.sample(X_train) #realiza uma amostragem aleatório do conjunto de dados\n",
        "print(f'Valor médio do modelo de regressão linear: {model.coef_ @ X_shap.mean() + model.intercept_}')\n",
        "explainer = shap.Explainer(model.predict,X_shap)\n",
        "shap_values = explainer(X_shap)\n",
        "#Shap value para a amostra 0\n",
        "shap.plots.waterfall(shap_values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-qitEnVJOWb"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values, X_shap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1QWSq0sJVd7"
      },
      "source": [
        "**b) Divida os dados em 80% para treino e treine um modelo de Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krvvjgsRJzu8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "X,y = shap.datasets.california(display=True)\n",
        "X_train,X_test = train_test_split(X,test_size=0.2, random_state=0)\n",
        "y_train,y_test = train_test_split(y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = ensemble.RandomForestRegressor(max_depth=5, random_state=0, n_estimators=5)\n",
        "model.fit(X_train,y_train)\n",
        "target_pred = model.predict(X_test)\n",
        "print(f\"RMSE do Random Forest: {metrics.mean_squared_error(y_test,target_pred)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xTksv9Yxfc4"
      },
      "outputs": [],
      "source": [
        "model_shap_values = shap.KernelExplainer(model.predict,X_shap)\n",
        "shap_values = model_shap_values.shap_values(X_shap)\n",
        "shap.summary_plot(shap_values, X_shap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgSIo8PKLBZD"
      },
      "outputs": [],
      "source": [
        "#Seleção de atributos na floresta\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "features = ['MedInc','HouseAge','AveRooms','AveBedrms','Population','AveOccup','Latitude','Longitude']\n",
        "plt.title('Importância dos atributos')\n",
        "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Importancia relativa')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# #avaliando diferentes permutações dos atributos\n",
        "# from sklearn.inspection import permutation_importance\n",
        "# result = permutation_importance(\n",
        "#     model, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2)\n",
        "\n",
        "# indices = np.argsort(result['importances_mean'])\n",
        "# plt.title('Importância dos atributos após 10 repetições')\n",
        "# plt.barh(range(len(indices)), result['importances_mean'][indices], color='b', align='center')\n",
        "# plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "# plt.xlabel('Importancia relativa')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eXPLFTZx8iJ"
      },
      "outputs": [],
      "source": [
        "#Treinando somente com os atributos selecionados MedInc, AveOccup, Latitude, HouseAge\n",
        "X_new = X.copy()\n",
        "\n",
        "X_new = X_new.drop(['AveRooms','AveBedrms','Population','Longitude'],axis = 1)\n",
        "\n",
        "X_train,X_test = train_test_split(X_new,test_size=0.2, random_state=0)\n",
        "y_train,y_test = train_test_split(y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = ensemble.RandomForestRegressor(max_depth=5, random_state=0, n_estimators=5)\n",
        "model.fit(X_train,y_train)\n",
        "target_pred = model.predict(X_test)\n",
        "print(f\"RMSE do Random Forest considerando só os atributos relevantes: {metrics.mean_squared_error(y_test,target_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPVdDibDRe5a"
      },
      "source": [
        "##K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8prgZ4WN42B"
      },
      "source": [
        "#**4) Realize o agrupamento de dados utilizando o k-means.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYfqUU9cSdgg"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('iris.data', header = None, names = ['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
        "\n",
        "#considera só dois atributos para visualização\n",
        "data = data.drop(['class', 'sepal length', 'sepal width'], axis=1)\n",
        "\n",
        "model = cluster.KMeans(n_clusters = 3, n_init = 10, max_iter = 100, random_state=0)\n",
        "output = model.fit_predict(data)\n",
        "\n",
        "# plot the 3 clusters\n",
        "plt.scatter(\n",
        "    data.iloc[output == 0, 0], data.iloc[output == 0, 1],\n",
        "    s=50, c='lightgreen',\n",
        "    marker='s', edgecolor='black',\n",
        "    label='cluster 1'\n",
        ")\n",
        "\n",
        "plt.scatter(\n",
        "    data.iloc[output == 1, 0], data.iloc[output == 1, 1],\n",
        "    s=50, c='orange',\n",
        "    marker='o', edgecolor='black',\n",
        "    label='cluster 2'\n",
        ")\n",
        "\n",
        "plt.scatter(\n",
        "    data.iloc[output == 2, 0], data.iloc[output == 2, 1],\n",
        "    s=50, c='lightblue',\n",
        "    marker='v', edgecolor='black',\n",
        "    label='cluster 3'\n",
        ")\n",
        "\n",
        "# plot the centroids\n",
        "plt.scatter(\n",
        "    model.cluster_centers_[:, 0], model.cluster_centers_[:, 1],\n",
        "    s=250, marker='*',\n",
        "    c='red', edgecolor='black',\n",
        "    label='centroids'\n",
        ")\n",
        "plt.legend(scatterpoints=1)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNA4FYmsVGGg"
      },
      "outputs": [],
      "source": [
        "#identificando o melhor valor de k\n",
        "distortions = []\n",
        "for i in range(1, 11):\n",
        "    model = cluster.KMeans(\n",
        "        n_clusters=i, init='random',\n",
        "        n_init=10, max_iter=100,\n",
        "        tol=1e-04, random_state=0\n",
        "    )\n",
        "    model.fit(data)\n",
        "    distortions.append(model.inertia_) #Sum of Squares Errors --> cost function J\n",
        "\n",
        "# plot\n",
        "plt.plot(range(1, 11), distortions, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.show()\n",
        "print('É escolhido k = 3, pois para k > 3 a redução da função de custo é pequena')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
